{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loading the packages that will be used\n",
    "list.of.packages <- c(\"tm\", \"dbscan\", \"proxy\", \"colorspace\")\n",
    "\n",
    "# (downloading and) requiring packages\n",
    "new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,\"Package\"])]\n",
    "if(length(new.packages)) \n",
    "  install.packages(new.packages)\n",
    "for (p in list.of.packages) \n",
    "  require(p, character.only = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rm(list = ls()) # Cleaning environment\n",
    "options(header = FALSE, stringsAsFactors = FALSE, fileEncoding = \"latin1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening notes on the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to cluster a dataset consisting of health news tweets. These short sentences belong to one of the 16 sources of news considered in the dataset. We are then facing a multi-label classifying problem, with `num_classes = 16`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "truth.K <- 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are about to download directly the data from the UCI Machine Learning repository. Thanks to native functions, we are able to download the zip file, extract it and fill a dataframe with all the text files read iteratively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the empty dataset with the formatted columns\n",
    "dataframe <- data.frame(ID=character(),\n",
    "                      datetime=character(),\n",
    "                      content=character(),\n",
    "                      label=factor())\n",
    "target.directory <- '/tmp/clustering-r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Download file\n",
    "#\n",
    "source.url <- 'https://archive.ics.uci.edu/ml/machine-learning-databases/00438/Health-News-Tweets.zip'\n",
    "temporary.file <- tempfile()\n",
    "download.file(source.url, temporary.file)\n",
    "unzip(temporary.file, exdir = target.directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Reading the files\n",
    "#\n",
    "target.directory <- paste(target.directory, 'Health-Tweets', sep = '/')\n",
    "files <- list.files(path = target.directory, pattern='.txt$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'bbchealth.txt'</li>\n",
       "\t<li>'cbchealth.txt'</li>\n",
       "\t<li>'cnnhealth.txt'</li>\n",
       "\t<li>'everydayhealth.txt'</li>\n",
       "\t<li>'foxnewshealth.txt'</li>\n",
       "\t<li>'gdnhealthcare.txt'</li>\n",
       "\t<li>'goodhealth.txt'</li>\n",
       "\t<li>'KaiserHealthNews.txt'</li>\n",
       "\t<li>'latimeshealth.txt'</li>\n",
       "\t<li>'msnhealthnews.txt'</li>\n",
       "\t<li>'NBChealth.txt'</li>\n",
       "\t<li>'nprhealth.txt'</li>\n",
       "\t<li>'nytimeshealth.txt'</li>\n",
       "\t<li>'reuters_health.txt'</li>\n",
       "\t<li>'usnewshealth.txt'</li>\n",
       "\t<li>'wsjhealth.txt'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'bbchealth.txt'\n",
       "\\item 'cbchealth.txt'\n",
       "\\item 'cnnhealth.txt'\n",
       "\\item 'everydayhealth.txt'\n",
       "\\item 'foxnewshealth.txt'\n",
       "\\item 'gdnhealthcare.txt'\n",
       "\\item 'goodhealth.txt'\n",
       "\\item 'KaiserHealthNews.txt'\n",
       "\\item 'latimeshealth.txt'\n",
       "\\item 'msnhealthnews.txt'\n",
       "\\item 'NBChealth.txt'\n",
       "\\item 'nprhealth.txt'\n",
       "\\item 'nytimeshealth.txt'\n",
       "\\item 'reuters\\_health.txt'\n",
       "\\item 'usnewshealth.txt'\n",
       "\\item 'wsjhealth.txt'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'bbchealth.txt'\n",
       "2. 'cbchealth.txt'\n",
       "3. 'cnnhealth.txt'\n",
       "4. 'everydayhealth.txt'\n",
       "5. 'foxnewshealth.txt'\n",
       "6. 'gdnhealthcare.txt'\n",
       "7. 'goodhealth.txt'\n",
       "8. 'KaiserHealthNews.txt'\n",
       "9. 'latimeshealth.txt'\n",
       "10. 'msnhealthnews.txt'\n",
       "11. 'NBChealth.txt'\n",
       "12. 'nprhealth.txt'\n",
       "13. 'nytimeshealth.txt'\n",
       "14. 'reuters_health.txt'\n",
       "15. 'usnewshealth.txt'\n",
       "16. 'wsjhealth.txt'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"bbchealth.txt\"        \"cbchealth.txt\"        \"cnnhealth.txt\"       \n",
       " [4] \"everydayhealth.txt\"   \"foxnewshealth.txt\"    \"gdnhealthcare.txt\"   \n",
       " [7] \"goodhealth.txt\"       \"KaiserHealthNews.txt\" \"latimeshealth.txt\"   \n",
       "[10] \"msnhealthnews.txt\"    \"NBChealth.txt\"        \"nprhealth.txt\"       \n",
       "[13] \"nytimeshealth.txt\"    \"reuters_health.txt\"   \"usnewshealth.txt\"    \n",
       "[16] \"wsjhealth.txt\"       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filling the dataframe by reading the text content\n",
    "for (f in files) {\n",
    "  news.filename = paste(target.directory , f, sep ='/')\n",
    "  news.label <- substr(f, 0, nchar(f) - 4) # Removing the 4 last characters => '.txt'\n",
    "  news.data <- read.csv(news.filename,\n",
    "                        encoding = 'UTF-8',\n",
    "                        header = FALSE,\n",
    "                        quote = \"\",\n",
    "                        sep = '|',\n",
    "                        col.names = c('ID', 'datetime', 'content'))\n",
    "  \n",
    "  # Trick to ignore last part of tweets which content contains the split character \"|\"\n",
    "  # No satisfying solution has been found to split (as in Python) and merging extra-columns with the last one\n",
    "  news.data <- news.data[news.data$content != \"\", ]\n",
    "  news.data['label'] = news.label # We add the label of the tweet \n",
    "  \n",
    "  # Only considering a little portion of data ...\n",
    "  # ... because handling sparse matrix for generic usage is a pain\n",
    "  news.data <- head(news.data, floor(nrow(news.data) * 0.05))\n",
    "  dataframe <- rbind(dataframe, news.data)\n",
    "}\n",
    "# Deleting the temporary directory\n",
    "unlink(target.directory, recursive =  TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences <- sub(\"http://([[:alnum:]|[:punct:]])+\", '', dataframe$content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in tm_map.SimpleCorpus(corpus, function(x) iconv(x, to = \"UTF-8\", :\n",
      "\"transformation drops documents\"Warning message in tm_map.SimpleCorpus(corpus.cleaned, tm::removeWords, tm::stopwords(\"english\")):\n",
      "\"transformation drops documents\"Warning message in tm_map.SimpleCorpus(corpus, tm::stemDocument, language = \"english\"):\n",
      "\"transformation drops documents\"Warning message in tm_map.SimpleCorpus(corpus.cleaned, tm::stripWhitespace):\n",
      "\"transformation drops documents\""
     ]
    }
   ],
   "source": [
    "corpus = tm::Corpus(tm::VectorSource(sentences))\n",
    "\n",
    "# Cleaning up\n",
    "\n",
    "# Handling UTF-8 encoding problem from the dataset\n",
    "corpus.cleaned <- tm::tm_map(corpus, function(x) iconv(x, to='UTF-8', sub='byte')) \n",
    "corpus.cleaned <- tm::tm_map(corpus.cleaned, tm::removeWords, tm::stopwords('english')) # Removing stop-words\n",
    "corpus.cleaned <- tm::tm_map(corpus, tm::stemDocument, language = \"english\") # Stemming the words \n",
    "corpus.cleaned <- tm::tm_map(corpus.cleaned, tm::stripWhitespace) # Trimming excessive whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Building the feature matrices\n",
    "tdm <- tm::DocumentTermMatrix(corpus.cleaned)\n",
    "tdm.tfidf <- tm::weightTfIdf(tdm)\n",
    "\n",
    "# We remove A LOT of features. R is natively very weak with high dimensional matrix\n",
    "tdm.tfidf <- tm::removeSparseTerms(tdm.tfidf, 0.999)\n",
    "\n",
    "# There is the memory-problem part\n",
    "# - Native matrix isn't \"sparse-compliant\" in the memory\n",
    "# - Sparse implementations aren't necessary compatible with clustering algorithms\n",
    "tfidf.matrix <- as.matrix(tdm.tfidf)\n",
    "# Cosine distance matrix (useful for specific clustering algorithms)\n",
    "dist.matrix = proxy::dist(tfidf.matrix, method = \"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<<DocumentTermMatrix (documents: 3159, terms: 7951)>>\n",
       "Non-/sparse entries: 30486/25086723\n",
       "Sparsity           : 100%\n",
       "Maximal term length: 62\n",
       "Weighting          : term frequency (tf)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Partitioning clustering**. As a partitioning clustering, we will use the famous K-means algorithm. As we know the dataset, we can define properly the number of awaited clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clustering.kmeans <- kmeans(tfidf.matrix, truth.K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hierarchical clustering**. R comes with an easy interface to run hierarchical clustering. All we have to define is the clustering criterion and the pointwise distance matrix. We will be using the Ward’s method as the clustering criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clustering.hierarchical <- hclust(dist.matrix, method = \"ward.D2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Density-based clustering**. To try the density-based clustering, we will run the HDBScan algorithm. We can run it easily from an external package, dbscan. Regarding the hyper-parameters of the algorithm, a more or less arbitrary value has been fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clustering.dbscan <- dbscan::hdbscan(dist.matrix, minPts = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
