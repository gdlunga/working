{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loading the packages that will be used\n",
    "list.of.packages <- c(\"tm\", \"dbscan\", \"proxy\", \"colorspace\")\n",
    "\n",
    "# (downloading and) requiring packages\n",
    "new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,\"Package\"])]\n",
    "if(length(new.packages)) \n",
    "  install.packages(new.packages)\n",
    "for (p in list.of.packages) \n",
    "  require(p, character.only = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rm(list = ls()) # Cleaning environment\n",
    "options(header = FALSE, stringsAsFactors = FALSE, fileEncoding = \"latin1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening notes on the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to cluster a dataset consisting of health news tweets. These short sentences belong to one of the 16 sources of news considered in the dataset. We are then facing a multi-label classifying problem, with `num_classes = 16`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "truth.K <- 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are about to download directly the data from the UCI Machine Learning repository. Thanks to native functions, we are able to download the zip file, extract it and fill a dataframe with all the text files read iteratively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the empty dataset with the formatted columns\n",
    "dataframe <- data.frame(ID=character(),\n",
    "                      datetime=character(),\n",
    "                      content=character(),\n",
    "                      label=factor())\n",
    "target.directory <- '/tmp/clustering-r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Download file\n",
    "#\n",
    "source.url <- 'https://archive.ics.uci.edu/ml/machine-learning-databases/00438/Health-News-Tweets.zip'\n",
    "temporary.file <- tempfile()\n",
    "download.file(source.url, temporary.file)\n",
    "unzip(temporary.file, exdir = target.directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Reading the files\n",
    "#\n",
    "target.directory <- paste(target.directory, 'Health-Tweets', sep = '/')\n",
    "files <- list.files(path = target.directory, pattern='.txt$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'bbchealth.txt'</li>\n",
       "\t<li>'cbchealth.txt'</li>\n",
       "\t<li>'cnnhealth.txt'</li>\n",
       "\t<li>'everydayhealth.txt'</li>\n",
       "\t<li>'foxnewshealth.txt'</li>\n",
       "\t<li>'gdnhealthcare.txt'</li>\n",
       "\t<li>'goodhealth.txt'</li>\n",
       "\t<li>'KaiserHealthNews.txt'</li>\n",
       "\t<li>'latimeshealth.txt'</li>\n",
       "\t<li>'msnhealthnews.txt'</li>\n",
       "\t<li>'NBChealth.txt'</li>\n",
       "\t<li>'nprhealth.txt'</li>\n",
       "\t<li>'nytimeshealth.txt'</li>\n",
       "\t<li>'reuters_health.txt'</li>\n",
       "\t<li>'usnewshealth.txt'</li>\n",
       "\t<li>'wsjhealth.txt'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'bbchealth.txt'\n",
       "\\item 'cbchealth.txt'\n",
       "\\item 'cnnhealth.txt'\n",
       "\\item 'everydayhealth.txt'\n",
       "\\item 'foxnewshealth.txt'\n",
       "\\item 'gdnhealthcare.txt'\n",
       "\\item 'goodhealth.txt'\n",
       "\\item 'KaiserHealthNews.txt'\n",
       "\\item 'latimeshealth.txt'\n",
       "\\item 'msnhealthnews.txt'\n",
       "\\item 'NBChealth.txt'\n",
       "\\item 'nprhealth.txt'\n",
       "\\item 'nytimeshealth.txt'\n",
       "\\item 'reuters\\_health.txt'\n",
       "\\item 'usnewshealth.txt'\n",
       "\\item 'wsjhealth.txt'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'bbchealth.txt'\n",
       "2. 'cbchealth.txt'\n",
       "3. 'cnnhealth.txt'\n",
       "4. 'everydayhealth.txt'\n",
       "5. 'foxnewshealth.txt'\n",
       "6. 'gdnhealthcare.txt'\n",
       "7. 'goodhealth.txt'\n",
       "8. 'KaiserHealthNews.txt'\n",
       "9. 'latimeshealth.txt'\n",
       "10. 'msnhealthnews.txt'\n",
       "11. 'NBChealth.txt'\n",
       "12. 'nprhealth.txt'\n",
       "13. 'nytimeshealth.txt'\n",
       "14. 'reuters_health.txt'\n",
       "15. 'usnewshealth.txt'\n",
       "16. 'wsjhealth.txt'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"bbchealth.txt\"        \"cbchealth.txt\"        \"cnnhealth.txt\"       \n",
       " [4] \"everydayhealth.txt\"   \"foxnewshealth.txt\"    \"gdnhealthcare.txt\"   \n",
       " [7] \"goodhealth.txt\"       \"KaiserHealthNews.txt\" \"latimeshealth.txt\"   \n",
       "[10] \"msnhealthnews.txt\"    \"NBChealth.txt\"        \"nprhealth.txt\"       \n",
       "[13] \"nytimeshealth.txt\"    \"reuters_health.txt\"   \"usnewshealth.txt\"    \n",
       "[16] \"wsjhealth.txt\"       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filling the dataframe by reading the text content\n",
    "for (f in files) {\n",
    "  news.filename = paste(target.directory , f, sep ='/')\n",
    "  news.label <- substr(f, 0, nchar(f) - 4) # Removing the 4 last characters => '.txt'\n",
    "  news.data <- read.csv(news.filename,\n",
    "                        encoding = 'UTF-8',\n",
    "                        header = FALSE,\n",
    "                        quote = \"\",\n",
    "                        sep = '|',\n",
    "                        col.names = c('ID', 'datetime', 'content'))\n",
    "  \n",
    "  # Trick to ignore last part of tweets which content contains the split character \"|\"\n",
    "  # No satisfying solution has been found to split (as in Python) and merging extra-columns with the last one\n",
    "  news.data <- news.data[news.data$content != \"\", ]\n",
    "  news.data['label'] = news.label # We add the label of the tweet \n",
    "  \n",
    "  # Only considering a little portion of data ...\n",
    "  # ... because handling sparse matrix for generic usage is a pain\n",
    "  news.data <- head(news.data, floor(nrow(news.data) * 0.05))\n",
    "  dataframe <- rbind(dataframe, news.data)\n",
    "}\n",
    "# Deleting the temporary directory\n",
    "unlink(target.directory, recursive =  TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences <- sub(\"http://([[:alnum:]|[:punct:]])+\", '', dataframe$content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in tm_map.SimpleCorpus(corpus, function(x) iconv(x, to = \"UTF-8\", :\n",
      "\"transformation drops documents\"Warning message in tm_map.SimpleCorpus(corpus.cleaned, tm::removeWords, tm::stopwords(\"english\")):\n",
      "\"transformation drops documents\"Warning message in tm_map.SimpleCorpus(corpus, tm::stemDocument, language = \"english\"):\n",
      "\"transformation drops documents\"Warning message in tm_map.SimpleCorpus(corpus.cleaned, tm::stripWhitespace):\n",
      "\"transformation drops documents\""
     ]
    }
   ],
   "source": [
    "corpus = tm::Corpus(tm::VectorSource(sentences))\n",
    "\n",
    "# Cleaning up\n",
    "\n",
    "# Handling UTF-8 encoding problem from the dataset\n",
    "corpus.cleaned <- tm::tm_map(corpus, function(x) iconv(x, to='UTF-8', sub='byte')) \n",
    "corpus.cleaned <- tm::tm_map(corpus.cleaned, tm::removeWords, tm::stopwords('english')) # Removing stop-words\n",
    "corpus.cleaned <- tm::tm_map(corpus, tm::stemDocument, language = \"english\") # Stemming the words \n",
    "corpus.cleaned <- tm::tm_map(corpus.cleaned, tm::stripWhitespace) # Trimming excessive whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Building the feature matrices\n",
    "tdm <- tm::DocumentTermMatrix(corpus.cleaned)\n",
    "tdm.tfidf <- tm::weightTfIdf(tdm)\n",
    "\n",
    "# We remove A LOT of features. R is natively very weak with high dimensional matrix\n",
    "tdm.tfidf <- tm::removeSparseTerms(tdm.tfidf, 0.999)\n",
    "\n",
    "# There is the memory-problem part\n",
    "# - Native matrix isn't \"sparse-compliant\" in the memory\n",
    "# - Sparse implementations aren't necessary compatible with clustering algorithms\n",
    "tfidf.matrix <- as.matrix(tdm.tfidf)\n",
    "# Cosine distance matrix (useful for specific clustering algorithms)\n",
    "dist.matrix = proxy::dist(tfidf.matrix, method = \"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<<DocumentTermMatrix (documents: 3159, terms: 7951)>>\n",
       "Non-/sparse entries: 30486/25086723\n",
       "Sparsity           : 100%\n",
       "Maximal term length: 62\n",
       "Weighting          : term frequency (tf)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Partitioning clustering**. As a partitioning clustering, we will use the famous K-means algorithm. As we know the dataset, we can define properly the number of awaited clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clustering.kmeans <- kmeans(tfidf.matrix, truth.K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hierarchical clustering**. R comes with an easy interface to run hierarchical clustering. All we have to define is the clustering criterion and the pointwise distance matrix. We will be using the Wardâ€™s method as the clustering criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clustering.hierarchical <- hclust(dist.matrix, method = \"ward.D2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Density-based clustering**. To try the density-based clustering, we will run the HDBScan algorithm. We can run it easily from an external package, dbscan. Regarding the hyper-parameters of the algorithm, a more or less arbitrary value has been fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clustering.dbscan <- dbscan::hdbscan(dist.matrix, minPts = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
